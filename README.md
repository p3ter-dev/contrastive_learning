# Text-Based Contrastive Learning for Semantic Similarity

## Project Goal

The primary objective of this project was to implement and evaluate a **Contrastive Learning** pipeline to fine-tune a pre-trained language model, enhancing its ability to generate high-quality **semantic embeddings**. The goal was to make the model capable of generating embeddings where questions with the same meaning (duplicates) are clustered closely in vector space, and semantically different questions are pushed apart.

This capability is essential for applications like **semantic search**, knowledge base **deduplication**, and improving **Information Retrieval (IR)** performance.

***

## Technology Stack & Dependencies
* **Core Libraries:** `sentence-transformers`, `transformers`, `datasets`, `umap-learn`, `matplotlib`, `gradio`.

***

## Data and Pair Construction

### Dataset
* **Source:** **Quora Question Pairs (QQP)** dataset (accessed via the stable GLUE benchmark subset).
* **Structure:** Pairs of questions (`question1`, `question2`) with a binary label (`label`): **1** for duplicates, **0** for non-duplicates.

### Positive and Hard Negative Pairs
1.  **Positive Pairs (Anchor-Positive):** Constructed from the `label=1` entries.
2.  **Hard Negative Mining:** The labeled non-duplicates (`label=0`) were used as explicit hard negatives, supplemented by **in-batch negatives** (other random questions in the batch) generated by the MNRL loss.

***

## Model Architecture and Objectives

### 1. The Encoder
* **Model:** **`all-MiniLM-L6-v2`**
* **Architecture:** Pre-trained Transformer Encoder, chosen for its strong performance relative to its size and speed.
* **Output:** 384-dimensional dense vector embeddings.

### 2. Contrastive Objective (Core Task)
* **Loss Function:** **Multiple Negatives Ranking Loss (MNRL)**
* **Mechanism:** MNRL implements the **InfoNCE** (Noise Contrastive Estimation) objective. It uses **Cosine Similarity** to maximize the score between an Anchor and its Positive sample while minimizing the score against all Negative samples in the batch.
* **Similarity Metric:** **Cosine Similarity** was the canonical distance metric used for all similarity calculations.

### 3. Multi-Task Extension
* **Auxiliary Loss:** **Cosine Similarity Loss**
* **Interaction:** This loss ran in parallel with MNRL, forcing the model to explicitly predict the binary duplication label (`0` or `1`) based on the cosine distance of the question pair embeddings.

***

## Results and Evaluation

### Benchmark Comparison

The model's performance was measured using **Mean Reciprocal Rank at 10 (MRR@10)**, a standard metric for retrieval quality.

| Metric | Baseline (Pre-trained MiniLM) | Fine-Tuned Model | Improvement |
| :--- | :--- | :--- | :--- |
| **MRR@10** | **0.4605** | **0.4557** | **-0.0048** |

### Embedding Space Visualization (UMAP)

The UMAP visualization showed the model's failure to learn the contrastive objective effectively.

* **Observation:** The high-dimensional embeddings, when reduced to 2D, showed **heavy intermixing** of the red (duplicate) and blue/purple (non-duplicate) points.
* **Interpretation:** The model failed to create the necessary **separation and tight clustering** required for effective semantic retrieval, leading to a slight performance degradation compared to the original model.

### Conclusion on Initial Run

The negative improvement and mixed UMAP clusters suggest the training failed to generalize, likely due to:
* Suboptimal **Hyperparameters** (e.g., small batch size for InfoNCE, which degrades the quality of "in-batch negatives").
* Poor **Loss Balance** between the primary MNRL loss and the auxiliary Classification Loss.

---

## Running and Next Steps

### Quick Test Interface

The Gradio interface provides a quick way to manually test the final model's output using Cosine Similarity:

```python
# To launch the interactive test:
iface.launch()